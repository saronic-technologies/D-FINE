__include__: [
  '../../dataset/custom_detection.yml',
  '../../runtime.yml',
  '../include/dataloader.yml',
  '../include/optimizer.yml',
  '../include/dfine_hgnetv2.yml',
]

output_dir: ./output/dfine_hgnetv2_l_custom


HGNetv2:
  name: 'B4'
  return_idx: [1, 2, 3]
  freeze_stem_only: True
  freeze_at: 0
  freeze_norm: True

optimizer:
  type: AdamW
  params:
    -
      params: '^(?=.*backbone)(?!.*norm|bn).*$'
      lr: 0.00005
    -
      params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$'
      weight_decay: 0.

  lr: 0.001
  betas: [0.9, 0.999]
  weight_decay: 0.0001


# ---------------------------------------------------------------------------
# LR scheduler
# ---------------------------------------------------------------------------
# Override the scheduler inherited from `include/optimizer.yml` so that the
# learning-rate is multiplied by `gamma` (0.1 ⇒ 10× decay) once the training
# reaches epoch 25.  This keeps the rest of the configuration intact while
# ensuring the decay happens inside the 28-epoch training window defined in
# this file.

lr_scheduler:
  # Linearly decay the learning-rate from its initial value down to
  # 0.1 × lr over the first 25 training epochs and keep it constant
  # afterwards.  The built-in ``LinearLR`` scheduler—available from
  # PyTorch 1.13 onward—is registered inside ``src/optim/optim.py`` so it can
  # be referenced directly from the YAML configuration.
  type: LinearLR
  start_factor: 1.0   # begin at the original learning-rate
  end_factor: 0.1     # reach 0.1 × lr
  total_iters: 25     # number of epochs for the decay phase


# Increase to search for the optimal ema
epochs: 28
train_dataloader:
  dataset:
    transforms:
      policy:
        epoch: 20
  collate_fn:
    stop_epoch: 20
    ema_restart_decay: 0.9999
    base_size_repeat: 4
